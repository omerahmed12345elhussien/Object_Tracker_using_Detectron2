{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerahmed12345elhussien/Object_Tracker_using_Detectron2/blob/main/Object_Tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Detectron2 Lab 3 : Building an Object Tracker\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "Welcome to detectron2!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "## Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_FzH13EjseR",
        "outputId": "e7c21411-d894-495c-fc4d-e174eaa7250b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp310-cp310-linux_x86_64.whl size=44090 sha256=271e59acd2de625ad12b39bda09b90fffc0dda0915aad75b4261cb6d4b75e7ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/83/31/975b737609aba39a4099d471d5684141c1fdc3404f97e7f68a\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2022.12.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\n",
            "flax 0.6.9 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyyaml-5.1\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15038, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 15038 (delta 29), reused 48 (delta 23), pack-reused 14975\u001b[K\n",
            "Receiving objects: 100% (15038/15038), 6.12 MiB | 6.94 MiB/s, done.\n",
            "Resolving deltas: 100% (10892/10892), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (5.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.3)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (3.3.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=65a86aae4ef6fda82b6188784da1a0323ce6fe28640d2f098f4bed175168ea44\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=2a7f97dd39dcf4f948bf8179b3ae1406b6cbad5b79f5f856c5c274ae55f19e68\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.3.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.1 portalocker-2.7.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REnw7ACbjBtT"
      },
      "source": [
        "### Required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-i4hmGYk1dL",
        "outputId": "de2058c7-7184-49b7-dac5-df8c15352645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "torch:  2.0 ; cuda:  cu118\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "import typing\n",
        "import torchvision.ops\n",
        "import os\n",
        "import joblib\n",
        "from torch import Tensor\n",
        "from detectron2.utils.colormap import random_color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfpjMr_nDQ_"
      },
      "source": [
        "\n",
        "## Part A: Download a video clip\n",
        "\n",
        "Download a small video clip of 41 frames from: https://github.com/gkioxari/aims2020_visualrecognition/releases/download/v1.0/videoclip.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfwV_Bg2n2Xq",
        "outputId": "a7e41772-ecbd-4b81-ddca-81b66c0b1ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-14 09:38:35--  https://github.com/gkioxari/aims2020_visualrecognition/releases/download/v1.0/videoclip.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/255177940/09ad9d80-7f47-11ea-93bc-002a89d4791c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230514T093835Z&X-Amz-Expires=300&X-Amz-Signature=a2d9ad6efde1e4738a4a9412ffc0a10ac0363a83217b56fa4ed46ab3284d0cf9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=255177940&response-content-disposition=attachment%3B%20filename%3Dvideoclip.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-05-14 09:38:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/255177940/09ad9d80-7f47-11ea-93bc-002a89d4791c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230514T093835Z&X-Amz-Expires=300&X-Amz-Signature=a2d9ad6efde1e4738a4a9412ffc0a10ac0363a83217b56fa4ed46ab3284d0cf9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=255177940&response-content-disposition=attachment%3B%20filename%3Dvideoclip.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1428660 (1.4M) [application/octet-stream]\n",
            "Saving to: ‘videoclip.zip’\n",
            "\n",
            "videoclip.zip       100%[===================>]   1.36M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-14 09:38:36 (36.0 MB/s) - ‘videoclip.zip’ saved [1428660/1428660]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download, decompress the video clip.\n",
        "!wget https://github.com/gkioxari/aims2020_visualrecognition/releases/download/v1.0/videoclip.zip\n",
        "!unzip videoclip.zip > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4I9nltn_ZfP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJuDo6OoywZe"
      },
      "source": [
        "##Part B: Object tracker class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvCOfcZMjruH"
      },
      "outputs": [],
      "source": [
        "class Object_Tracker:\n",
        "\n",
        "  def __init__(self,folder_path:str = None, model_thresh:float=0.5,overlap_thresh:float=0.8,start_idx:int=0,end_idx:int=10):\n",
        "    \"\"\"\n",
        "    The class constructor.\n",
        "    Input:\n",
        "    - folder_path: The folder path for our images.\n",
        "    - model_thresh: A threshold of the model for the tasks of object detection and instance segmentation.\n",
        "    - overlap_thresh: A threshold value we set to compare two overlapping boxes.\n",
        "    - start_idx: Index of the first frame we need to consider in our implementation.\n",
        "    - end_idx: Index of the last frame we need to consider in our implementation.\n",
        "    Example: We design our code with a default value of start_idx=0, end_idx=10, which will take the first 10 frames.\n",
        "    While, changing the values start_idx and end_idx will give us access to several parts in our folder since we built the\n",
        "    code on more than 10 frames.\n",
        "    \"\"\"\n",
        "    self.folder_path=folder_path\n",
        "    self.model_thresh=model_thresh\n",
        "    self.overlap_thresh=overlap_thresh\n",
        "    self.start_idx=start_idx\n",
        "    self.end_idx=end_idx\n",
        "    pass\n",
        "\n",
        "  def __read_files(self):\n",
        "    \"\"\"\n",
        "    __read_files() reads the files in the self.folder_path.\n",
        "    It creates a class attribute self.images_path: which consists of the names of the files sorted in ascending order.\n",
        "    \"\"\"\n",
        "    assert type(self.folder_path)==str, \"The folder path should be string.\"\n",
        "    # folder path\n",
        "    dir_path = repr(self.folder_path)[1:-1]\n",
        "    # list to store files\n",
        "    images_path = []\n",
        "    # Iterate directory\n",
        "    for path in os.listdir(dir_path):\n",
        "      if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        images_path.append(os.path.join(dir_path, path))\n",
        "    #Sor the files ascending\n",
        "    images_path.sort()\n",
        "    assert len(images_path)>=(self.end_idx-self.start_idx), \"The default length is 10, please change self.start_idx and self.end_idx for your current size.\"\n",
        "    assert self.start_idx>=0 and type(self.start_idx)==int and self.end_idx>1 and type(self.end_idx)==int and self.end_idx>(self.start_idx+1)\n",
        "    self.images_path=images_path[self.start_idx:self.end_idx]\n",
        "\n",
        "  def __model_prediction(self):\n",
        "    \"\"\"\n",
        "    __model_prediction(): implements the bounding box and instance segmentation tasks relying on R-CNN model with\n",
        "    a ResNet-101-FPN backbone pre-trained on the COCO dataset. We made the model fixed for the simplicity of our\n",
        "    implementation.\n",
        "    It creates class attributes: self.cfg, and self.results.\n",
        "    \"\"\"\n",
        "    assert self.model_thresh>0 and self.model_thresh<1 and type(self.model_thresh)==float, \"model threshold should be in the range (0,1)\"\n",
        "    #Initialize the results list to stor the outputs of our predictions.\n",
        "    self.results=[]\n",
        "    for image1 in self.images_path:\n",
        "      im = cv2.imread(image1)\n",
        "      self.cfg = get_cfg()\n",
        "      # project-specific config\n",
        "      self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
        "      self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.model_thresh  # threshold for this model\n",
        "      # Set the weights\n",
        "      self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
        "      predictor = DefaultPredictor(self.cfg)\n",
        "      outputs = predictor(im)\n",
        "      self.results.append(outputs)\n",
        "\n",
        "  def matching_score(self,P: dict,Q: dict):\n",
        "    \"\"\"\n",
        "    matching_score() implements the scoring function between prediction P and Q. It considers two prediction are similar if they are\n",
        "    from the same class and that their overlapping boxes value is more than self.overlap_thresh.\n",
        "    It creates a class attribute self.indexes, which consists of the indexes of the matches in P and Q.\n",
        "\n",
        "    \"\"\"\n",
        "    assert self.overlap_thresh>0 and self.overlap_thresh<1 and type(self.overlap_thresh)==float, \"overlap threshold should be in the range (0,1)\"\n",
        "    #Initialize the index lists for P and Q.\n",
        "    idx_P,idx_Q=[],[]\n",
        "    for idx1,valu1 in enumerate(P[\"instances\"].pred_boxes):\n",
        "      max_list=torch.zeros(Q[\"instances\"].pred_boxes.tensor.size(0))\n",
        "      for idx2, valu2 in enumerate(Q[\"instances\"].pred_boxes):\n",
        "        #Check the condition that P_i and Q_i are from the same class and they have overlapping boxes with value more than self.overlap_thresh\n",
        "        if (P[\"instances\"].pred_classes[idx1]==Q[\"instances\"].pred_classes[idx2]) and (torchvision.ops.box_iou(valu1.view(1,-1),valu2.view(1,-1)).item()>self.overlap_thresh ):\n",
        "          #Compute IoU for P_i and Q_i\n",
        "          max_list[idx2]=torchvision.ops.box_iou(valu1.view(1,-1),valu2.view(1,-1)).item()\n",
        "        else:\n",
        "          max_list[idx2]=0\n",
        "      if torch.max(max_list)!=0:\n",
        "        idx_Q.append(torch.argmax(max_list).item())\n",
        "        idx_P.append(idx1)\n",
        "    self.indexes=torch.tensor([idx_P,idx_Q])\n",
        "\n",
        "  def __set_colors(self,val_1:int,val_2:int,idxs:Tensor,base_color:list=None)->tuple:\n",
        "    \"\"\"\n",
        "    __set_colors() is used to set the color for our frames.\n",
        "    Input:\n",
        "    - val_1: The number of objects in frame 1.\n",
        "    - val_2: The number of objects in frame 2.\n",
        "    - idxs: The indexes for the current frame, which is one row of self.indexes.\n",
        "    - base_color: The color of the previous frame. We use it, since we may have different number of\n",
        "    objects in the two frames.\n",
        "    \"\"\"\n",
        "    if base_color:\n",
        "      #The case that frame 1 has more objects than frame 2.\n",
        "      if max(val_1,val_2)==val_1:\n",
        "        #Set the colors for frame 2.\n",
        "        assig_color_min=[random_color(rgb=True, maximum=1)  for _ in range(val_2)]\n",
        "        for i_1,i_2 in zip(idxs[0],idxs[1]):\n",
        "          #Change the colors of our trackers in frame 2 relying on base_color.\n",
        "          assig_color_min[i_2.item()]=base_color[i_1.item()]\n",
        "        return base_color,assig_color_min\n",
        "      else:\n",
        "        #Set the colors for frame 2.\n",
        "        assig_color_max=[random_color(rgb=True, maximum=1)  for _ in range(val_2)]\n",
        "        for i_1,i_2 in zip(idxs[0],idxs[1]):\n",
        "          #Change the colors of our trackers in frame 2 relying on base_color.\n",
        "          assig_color_max[i_2.item()]=base_color[i_1.item()]\n",
        "        return assig_color_max,base_color\n",
        "\n",
        "    else:\n",
        "      #Set the colors for frame 1 and 2.\n",
        "      assig_color_max=[random_color(rgb=True, maximum=1)  for _ in range(max(val_1,val_2))]\n",
        "      assig_color_min=assig_color_max[0:min(val_1,val_2)].copy()\n",
        "      #The case when we don't have trackers.\n",
        "      if idxs.size(0)==0:\n",
        "        return assig_color_max,assig_color_min\n",
        "      #The case when frame 1 is larger than frame 2.\n",
        "      elif max(val_1,val_2)==val_1:\n",
        "        for i,j in zip(idxs[0],idxs[1]):\n",
        "          assig_color_min[j.item()]=assig_color_max[i.item()]\n",
        "\n",
        "        return assig_color_max,assig_color_min\n",
        "      #The case when frame 1 is smaller than frame 2\n",
        "      else:\n",
        "        for i,j in zip(idxs[0],idxs[1]):\n",
        "          assig_color_min[i.item()]=assig_color_max[j.item()]\n",
        "\n",
        "        return assig_color_max,assig_color_min\n",
        "\n",
        "  def __update_labels(self,labels:list, idxs:Tensor,pass_label:list =None, pass_track:list =None, count:int=None,part_two:bool=False)->tuple:\n",
        "    \"\"\"\n",
        "    __update_labels() is used to update the labels of our frame.\n",
        "    Ipute:\n",
        "    - labels: Our standard label without changing the name for some of them to track1, track2, etc.\n",
        "    - idxs: The indexes for the current frame, which is one row of self.indexes.\n",
        "    - pass_label: The labels of the previous frame.\n",
        "    - pass_track: The indexes of the previous frame similar to idxs.\n",
        "    - count: The number of trackers we have so far.\n",
        "    - part_two: Is the current update for the second frame in the same matching, or is it for two frames in different matches.\n",
        "    \"\"\"\n",
        "    #The case when there is no matches between the two frames.\n",
        "    if idxs.size(0)==0:\n",
        "      return labels,count\n",
        "    elif pass_label:\n",
        "      #The same number of trackers for two different matching.\n",
        "      if idxs.size(0)==pass_track.size(0):\n",
        "        for i,j in zip(idxs,pass_track):\n",
        "          #The case when we have the same tracker in the next comparison.\n",
        "          if (i==j) and part_two==False:\n",
        "            labels[i.item()]=pass_label[j.item()]\n",
        "          #The case when the tracker idex changed.\n",
        "          elif (i!=j) and part_two==False:\n",
        "            labels[i.item()]=\"track\"+ str(count+1)\n",
        "            count+=1\n",
        "          #The case of part_two=True, where we only copy from pass_label\n",
        "          else:\n",
        "            labels[i.item()]=pass_label[j.item()]\n",
        "        return labels,count\n",
        "      #The case where we have more or less trackers compared with the passed number of trackers.\n",
        "      else:\n",
        "        for i in idxs:\n",
        "          if i in pass_track:\n",
        "            labels[i.item()]=pass_label[i.item()]\n",
        "          else:\n",
        "            labels[i.item()]=\"track\"+ str(count+1)\n",
        "            count+=1\n",
        "        return labels,count\n",
        "    #The first initialization.\n",
        "    elif count==0:\n",
        "      tracker_label= [\"track\"+ str(s) for s in range(1,1+idxs.size(0))]\n",
        "      saved_count=idxs.size(0)\n",
        "      for id,value in enumerate(idxs):\n",
        "        labels[value.item()]=tracker_label[id]\n",
        "      return labels,saved_count\n",
        "    pass\n",
        "  def visualizer_fun(self,frame_1: dict, frame_2: dict, image_path_idx, pass_color:list =None, pass_track:Tensor = None, pass_label:list =None,count:int=None)->tuple:\n",
        "    \"\"\"\n",
        "    visualizer_fun() implements tracking objects for two frames.\n",
        "    Input:\n",
        "    - frame_1: The first frame prediction.\n",
        "    - frame_2: The second frame prediction.\n",
        "    - image_path_idx: The index of the original image for frame 1, and frame 2.\n",
        "    - pass_color: Passed color from the previous comparison of our current frame 1 and another frame.\n",
        "    - pass_track: The indexes of frame 1, when compared with another frame in the previous comparison.\n",
        "    - pass_label: The labels of frame 1, when compared with another frame in the previous comparison.\n",
        "    - count: The number of trackers that we have so far.\n",
        "    \"\"\"\n",
        "    #Compute matching score.\n",
        "    self.matching_score(frame_1,frame_2)\n",
        "    #Determine the number of objects.\n",
        "    val_1=frame_1[\"instances\"].pred_boxes.tensor.size(0)\n",
        "    val_2=frame_2[\"instances\"].pred_boxes.tensor.size(0)\n",
        "    assig_color_max,assig_color_min=self.__set_colors(val_1,val_2,self.indexes,pass_color)\n",
        "\n",
        "    count1=0\n",
        "    for (outputs, track,idx) in  zip([frame_1,frame_2], self.indexes,image_path_idx):\n",
        "      if(outputs[\"instances\"].pred_boxes.tensor.size(0)==len(assig_color_max)):\n",
        "        color_assig=assig_color_max\n",
        "      else:\n",
        "        color_assig=assig_color_min\n",
        "      #Initialize the standard labels.\n",
        "      labels1=[MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).thing_classes[c]+\" \"+ \"{:.0f}%\".format(s * 100)  for c,s in zip(outputs[\"instances\"].pred_classes,outputs[\"instances\"].scores)]\n",
        "      if count1==0:\n",
        "        labels1,saved_count=self.__update_labels(labels1,track, pass_label,pass_track,count)\n",
        "        old_labels=labels1\n",
        "        old_track=track\n",
        "        count1+=1\n",
        "      else:\n",
        "        labels1,saved_count=self.__update_labels(labels1,track,old_labels,old_track,saved_count,True)\n",
        "      im = cv2.imread(self.images_path[idx])\n",
        "      v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "      #image object.\n",
        "      out = v.overlay_instances(boxes=outputs[\"instances\"].pred_boxes.to(\"cpu\"),\n",
        "            labels=labels1,\n",
        "            masks=outputs[\"instances\"].pred_masks.to(\"cpu\"),\n",
        "            keypoints=None,\n",
        "            assigned_colors=color_assig,\n",
        "            alpha=0.5,)\n",
        "      cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "    return color_assig,labels1, track, saved_count\n",
        "  def visuali_fun_videos(self):\n",
        "    \"\"\"\n",
        "    visuali_fun_videos() track objects for a long sequence of frames rather than two frames.\n",
        "    \"\"\"\n",
        "    #Read the files\n",
        "    self.__read_files()\n",
        "    #Make model prediction\n",
        "    self.__model_prediction()\n",
        "    for i,j in zip(range(len(self.results)-1),range(self.start_idx,self.end_idx-1)):\n",
        "      if i==0:\n",
        "        color_assig,tracker_label, track, saved_count=self.visualizer_fun(self.results[i],self.results[i+1],[j,j+1],count=0)\n",
        "      else:\n",
        "        color_assig,tracker_label, track,saved_count=self.visualizer_fun(self.results[i],self.results[i+1],[j,j+1],color_assig,track,tracker_label,saved_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REZJxZbzdMTV"
      },
      "source": [
        "We got our results in the report using: model_thresh=0.5,overlap_thresh=0.8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZUCPFF2clfK0"
      },
      "outputs": [],
      "source": [
        "#To get the same results of the report.\n",
        "ot=Object_Tracker(\"/content/clip/\")\n",
        "ot.visuali_fun_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDt4y1mWIuh6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMPlTBv7YOn0"
      },
      "source": [
        "## Documentation (How to implement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKC6PFEMLuXs"
      },
      "source": [
        "When you upload your zipped folder, make sure the files are named in this way(00.jpg, 01.jpg, 02.jpg, .etc) since we sort them during our implementation (00.jpg: means the first frame, 01.jpg: means the second frame, and so on).\n",
        "\n",
        "Steps to follow during implementation:\n",
        "\n",
        "1- Starts the notebook by installing detectron first. Then, run the remaining cells in part (Install detectron2), for importing required packages.\n",
        "\n",
        "2- In part (B), run the cell of Object_Tracker class.\n",
        "\n",
        "3- To get the same results as we got in our report, you can run the cell for downolading videoclip.zip in part(A) with the cell after the class.\n",
        "\n",
        "4- In the below two cells: change the name of (file_name.zip) to the name of the file you upload; then, copy the path of your unzipped folder. Change the name of (folder_path) with the path you copied.\n",
        "\n",
        "5- Run the two cells.\n",
        "\n",
        "Note: these are parameters that can be changed in the class object (model_thresh=0.5,overlap_thresh=0.8,start_idx=0,end_idx=10).\n",
        "\n",
        "- For instance, if you upload a file with more than 10 images, the code as default will consider the first ten images. You can expand the code to work on all of them or some of them by changing start_idx and end_idx.\n",
        "- overlap_thresh: can be changed in the range of (0,1) with the default value 0.8.\n",
        "\n",
        "- model_thresh: can  be changed which is the threshold score we pass to our model; default value is 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3uPDkukYcCH"
      },
      "outputs": [],
      "source": [
        "#Change the file_name.zip with the name of the folder you upload.\n",
        "!unzip file_name.zip > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufU_cglNYb5F"
      },
      "outputs": [],
      "source": [
        "#Add your folder path here.\n",
        "folder_path=\"/your_folder_path/folder_name/\"\n",
        "ot=Object_Tracker(folder_path)\n",
        "ot.visuali_fun_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMi9nbsVYbT8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vM54r6jlKTII",
        "UzfpjMr_nDQ_",
        "rJuDo6OoywZe",
        "CMPlTBv7YOn0"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}